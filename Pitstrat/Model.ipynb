{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ANN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mANN\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPModel , F1Dataset, NN_Eval\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ANN'"
     ]
    }
   ],
   "source": [
    "from ANN import MLPModel , F1Dataset, NN_Eval\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=pd.read_csv('Processed_DATA_PITSTRAT.csv')\n",
    "\n",
    "def preprocess(df_cleaned):\n",
    "\n",
    "    input_data = df_cleaned[['circuitId', 'raceId', 'points', 'lap','position','laps']].values\n",
    "    target_data = df_cleaned['stop'].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    input_data[:, 2:] = scaler.fit_transform(input_data[:, 2:])\n",
    "\n",
    "    circuit_encoded = F.one_hot(torch.tensor(input_data[:, 0],dtype=torch.long), num_classes=81)\n",
    "    race_encoded = F.one_hot(torch.tensor(input_data[:, 1],dtype=torch.long), num_classes=1133)\n",
    "    points = torch.tensor(input_data[:, 2], dtype=torch.float32).unsqueeze(1)\n",
    "    lap = torch.tensor(input_data[:, 3], dtype=torch.float32).unsqueeze(1)\n",
    "    position = torch.tensor(input_data[:, 4], dtype=torch.float32).unsqueeze(1)\n",
    "    laps = torch.tensor(input_data[:, 5], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "    input_data_encoded = torch.cat([circuit_encoded, race_encoded, points, lap,position,laps], dim=1)\n",
    "    target_encoded = F.one_hot(torch.tensor(target_data,dtype=torch.long), num_classes=8)\n",
    "    return input_data_encoded,target_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "input_data_encoded,target_encoded=preprocess(df_cleaned)\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data_encoded, target_encoded, test_size=0.2, random_state=42)\n",
    "train_dataset = F1Dataset(X_train, y_train)\n",
    "test_dataset = F1Dataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "best_loss = float('inf')\n",
    "circuit_id_max = 81\n",
    "race_id_max = 1133\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "weight_decay = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 1.3041 Validation loss: 0.8324\n",
      "Epoch [2/80], Loss: 0.7428 Validation loss: 0.5814\n",
      "Epoch [3/80], Loss: 0.5827 Validation loss: 0.4596\n",
      "Epoch [4/80], Loss: 0.5078 Validation loss: 0.4439\n",
      "Epoch [5/80], Loss: 0.4623 Validation loss: 0.4564\n",
      "Epoch [6/80], Loss: 0.4385 Validation loss: 0.4063\n",
      "Epoch [7/80], Loss: 0.4056 Validation loss: 0.3838\n",
      "Epoch [8/80], Loss: 0.3934 Validation loss: 0.4035\n",
      "Epoch [9/80], Loss: 0.4071 Validation loss: 0.4080\n",
      "Epoch [10/80], Loss: 0.3620 Validation loss: 0.3977\n",
      "Epoch [11/80], Loss: 0.3652 Validation loss: 0.4309\n",
      "Epoch [12/80], Loss: 0.3628 Validation loss: 0.3841\n",
      "Epoch [13/80], Loss: 0.3545 Validation loss: 0.3919\n",
      "Epoch [14/80], Loss: 0.3605 Validation loss: 0.4249\n",
      "Epoch [15/80], Loss: 0.3516 Validation loss: 0.3907\n",
      "Epoch [16/80], Loss: 0.3146 Validation loss: 0.4597\n",
      "Epoch [17/80], Loss: 0.3345 Validation loss: 0.4276\n",
      "Epoch [18/80], Loss: 0.3197 Validation loss: 0.3728\n",
      "Epoch [19/80], Loss: 0.3202 Validation loss: 0.4230\n",
      "Epoch [20/80], Loss: 0.3025 Validation loss: 0.4301\n",
      "Epoch [21/80], Loss: 0.3076 Validation loss: 0.3923\n",
      "Epoch [22/80], Loss: 0.2887 Validation loss: 0.4264\n",
      "Epoch [23/80], Loss: 0.2988 Validation loss: 0.4107\n",
      "Epoch [24/80], Loss: 0.2890 Validation loss: 0.4237\n",
      "Epoch [25/80], Loss: 0.3053 Validation loss: 0.3842\n",
      "Epoch [26/80], Loss: 0.2936 Validation loss: 0.4268\n",
      "Epoch [27/80], Loss: 0.2885 Validation loss: 0.4197\n",
      "Epoch [28/80], Loss: 0.2821 Validation loss: 0.4143\n",
      "Epoch [29/80], Loss: 0.3121 Validation loss: 0.4308\n",
      "Epoch [30/80], Loss: 0.3111 Validation loss: 0.4131\n",
      "Epoch [31/80], Loss: 0.2883 Validation loss: 0.4464\n",
      "Epoch [32/80], Loss: 0.3283 Validation loss: 0.4091\n",
      "Epoch [33/80], Loss: 0.3182 Validation loss: 0.4187\n",
      "Early stopping at epoch 33 with patience 15 reached.\n"
     ]
    }
   ],
   "source": [
    "net = MLPModel(circuit_id_max=circuit_id_max, race_id_max=race_id_max,dropout_prob=0.2).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "Evaluate_ANN=NN_Eval(net,train_loader,test_loader)\n",
    "net=Evaluate_ANN.train_model( optimizer, criterion, epochs=80, patience=15).model\n",
    "\n",
    "print(net())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "katana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
