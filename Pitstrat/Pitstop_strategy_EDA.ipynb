{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Download directory\n",
    "dir=\"../Dataset_Kaggle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, r2_score,precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frames\n",
    "\n",
    "df_stops=pd.read_csv(dir+\"/pit_stops.csv\")\n",
    "df_constructor_standing=pd.read_csv(dir+\"/constructor_standings.csv\")\n",
    "df_races=pd.read_csv(dir+\"/races.csv\")\n",
    "df_result=pd.read_csv(dir+\"/results.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing for merging\n",
    "\n",
    "\n",
    "df_result.drop(columns=['time','milliseconds'],axis=1,inplace=True)\n",
    "df_constructor_standing.drop(columns=['position','positionText','points'],axis=1,inplace=True)\n",
    "df_races.drop(columns=['time'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['raceId', 'driverId', 'stop', 'lap', 'time', 'duration', 'milliseconds',\n",
      "       'resultId', 'constructorId', 'number', 'grid', 'position',\n",
      "       'positionText', 'positionOrder', 'points', 'laps', 'fastestLap', 'rank',\n",
      "       'fastestLapTime', 'fastestLapSpeed', 'statusId',\n",
      "       'constructorStandingsId', 'wins'],\n",
      "      dtype='object')\n",
      "   raceId  driverId  stop  lap      time duration  milliseconds  resultId  \\\n",
      "0     841       153     1    1  17:05:23   26.898         26898     20789   \n",
      "1     841        30     1    1  17:05:52   25.021         25021     20797   \n",
      "2     841        17     1   11  17:20:48   23.426         23426     20783   \n",
      "3     841         4     1   12  17:22:34   23.251         23251     20782   \n",
      "4     841        13     1   13  17:24:10   23.842         23842     20785   \n",
      "\n",
      "   constructorId number  ...  fp1_date fp1_time fp2_date  fp2_time  fp3_date  \\\n",
      "0              5     19  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "1            131      7  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "2              9      2  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "3              6      5  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "4              6      6  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "\n",
      "   fp3_time quali_date quali_time sprint_date sprint_time  \n",
      "0        \\N         \\N         \\N          \\N          \\N  \n",
      "1        \\N         \\N         \\N          \\N          \\N  \n",
      "2        \\N         \\N         \\N          \\N          \\N  \n",
      "3        \\N         \\N         \\N          \\N          \\N  \n",
      "4        \\N         \\N         \\N          \\N          \\N  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "#Merge Frames\n",
    "first_merge_cols=[\"raceId\",\"driverId\"]\n",
    "commmon_columns_CS_PT=[\"raceId\",'constructorId']\n",
    "commmon_columns_CS_PT_RC=[\"raceId\"]\n",
    "merged_CS_PT=pd.merge(df_stops,df_result,on=first_merge_cols,how=\"inner\")\n",
    "merged_CS_PT=pd.merge(merged_CS_PT,df_constructor_standing,on=commmon_columns_CS_PT,how=\"inner\")\n",
    "merged_CS_PT_RC=pd.merge(merged_CS_PT,df_races,on=commmon_columns_CS_PT_RC,how=\"inner\")\n",
    "print(merged_CS_PT.columns)\n",
    "print(merged_CS_PT_RC.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   raceId  driverId  stop  lap      time duration  milliseconds  resultId  \\\n",
      "0     841       153     1    1  17:05:23   26.898         26898     20789   \n",
      "1     841        30     1    1  17:05:52   25.021         25021     20797   \n",
      "2     841        17     1   11  17:20:48   23.426         23426     20783   \n",
      "3     841         4     1   12  17:22:34   23.251         23251     20782   \n",
      "4     841        13     1   13  17:24:10   23.842         23842     20785   \n",
      "\n",
      "   constructorId number  ...  fp1_date fp1_time fp2_date  fp2_time  fp3_date  \\\n",
      "0              5     19  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "1            131      7  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "2              9      2  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "3              6      5  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "4              6      6  ...        \\N       \\N       \\N        \\N        \\N   \n",
      "\n",
      "   fp3_time quali_date quali_time sprint_date sprint_time  \n",
      "0        \\N         \\N         \\N          \\N          \\N  \n",
      "1        \\N         \\N         \\N          \\N          \\N  \n",
      "2        \\N         \\N         \\N          \\N          \\N  \n",
      "3        \\N         \\N         \\N          \\N          \\N  \n",
      "4        \\N         \\N         \\N          \\N          \\N  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_CS_PT_RC.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   raceId  driverId  stop  lap  resultId  constructorId  grid  position  \\\n",
      "0     841       153     1    1     20789              5    12      11.0   \n",
      "2     841        17     1   11     20783              9     3       5.0   \n",
      "3     841         4     1   12     20782              6     5       4.0   \n",
      "4     841        13     1   13     20785              6     8       7.0   \n",
      "6     841        20     1   14     20779              9     1       1.0   \n",
      "\n",
      "   points  laps  rank  statusId  wins  year  circuitId  \n",
      "0     0.0    57    10        11     0  2011          1  \n",
      "2    10.0    58     3         1     1  2011          1  \n",
      "3    12.0    58     2         1     0  2011          1  \n",
      "4     6.0    58     1         1     0  2011          1  \n",
      "6    25.0    58     4         1     1  2011          1  \n",
      "(10061, 15)\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning\n",
    "\n",
    "df_cleaned=merged_CS_PT_RC.drop(columns=['duration'])\n",
    "\n",
    "\n",
    "# nan_count = merged_CS_PT_RC.isna().sum()\n",
    "# print(nan_count)\n",
    "\n",
    "# rows_with_nan = merged_CS_PT_RC[merged_CS_PT_RC.isna().any(axis=1)]\n",
    "# print(rows_with_nan)\n",
    "df_cleaned=merged_CS_PT_RC.drop(columns=['constructorStandingsId','name','positionOrder','positionText','fastestLap','fastestLapTime','fastestLapSpeed','time','milliseconds','number','duration',\"round\",\"date\",\"url\",\"fp1_date\",\"fp1_time\",\"fp3_time\",\"quali_date\",\"fp2_date\",\"fp2_time\",\"fp3_date\",\"quali_time\",\"sprint_date\",\"sprint_time\"])\n",
    "# nan_count = merged_CS_PT_RC.isna().sum()\n",
    "# print(nan_count)\n",
    "df_cleaned.replace('\\\\N', np.nan, inplace=True)\n",
    "df_cleaned['position'] = pd.to_numeric(df_cleaned['position'], errors='coerce')\n",
    "df_cleaned['rank'] = pd.to_numeric(df_cleaned['rank'], errors='coerce')\n",
    "\n",
    "df_sampled=df_cleaned.sample(n=500,random_state=42)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Print all rows containing NaN values\n",
    "df_cleaned.dropna(inplace=True)\n",
    "# print(df_cleaned.describe())\n",
    "print(df_cleaned.head())\n",
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visuilazing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df_sampled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import math\n",
    "\n",
    "# num_plots = len(df_cleaned.columns)\n",
    "\n",
    "# n_cols = math.ceil(num_plots / 2) \n",
    "# n_rows = 2\n",
    "\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*5), constrained_layout=True)\n",
    "\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for idx, col in enumerate(df_cleaned.columns):\n",
    "#     sns.violinplot(data=df_sampled, y=col, ax=axes[idx])  \n",
    "#     axes[idx].set_title(f'Violin Plot of {col}')  \n",
    "\n",
    "# if num_plots < len(axes):\n",
    "#     for i in range(num_plots, len(axes)):\n",
    "#         fig.delaxes(axes[i]) \n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correlation_matrix = df_cleaned.corr()\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.heatmap(correlation_matrix,annot=True,cmap='seismic',linewidths=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "# df_sampled=df_cleaned.sample(n=5000,random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "df_grouped = df_cleaned[df_cleaned['points'] > 0].groupby(['constructorId', 'laps']).mean().reset_index()\n",
    "# for constructor_id in df_grouped['constructorId'].unique():\n",
    "#     constructor_data = df_grouped[df_grouped['constructorId'] == constructor_id]\n",
    "#     plt.scatter(constructor_data['year'], constructor_data['lap'], label=f'Constructor {constructor_id}')\n",
    "\n",
    "# # Adding plot labels and title\n",
    "# plt.title('Average Pit Stop Lap by Constructor')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Pit Lap')\n",
    "# plt.legend(title='Constructor ID')\n",
    "# plt.grid(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for constructor_id in df_grouped['constructorId'].unique():\n",
    "#     constructor_data = df_grouped[df_grouped['constructorId'] == constructor_id]\n",
    "#     plt.scatter(constructor_data['year'], constructor_data['stop'], label=f'Constructor {constructor_id}')\n",
    "\n",
    "# # Adding plot labels and title\n",
    "# plt.title('Average number of pit stops by Constructor')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('No of stops')\n",
    "# plt.legend(title='Constructor ID')\n",
    "# plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            raceId     driverId         stop         lap      resultId  \\\n",
      "count  2661.000000  2661.000000  2661.000000  2661.00000   2661.000000   \n",
      "mean    977.502818   383.894401     1.728298    26.24239  23549.499436   \n",
      "std      88.150864   406.975514     0.893283    14.59172   1705.699434   \n",
      "min     841.000000     1.000000     1.000000     1.00000  20779.000000   \n",
      "25%     897.000000     4.000000     1.000000    14.00000  22066.000000   \n",
      "50%     970.000000    20.000000     2.000000    26.00000  23403.000000   \n",
      "75%    1055.000000   830.000000     2.000000    36.00000  25050.000000   \n",
      "max    1132.000000   857.000000     6.000000    70.00000  26509.000000   \n",
      "\n",
      "       constructorId         grid     position       points         laps  \\\n",
      "count    2661.000000  2661.000000  2661.000000  2661.000000  2661.000000   \n",
      "mean       51.306652     4.788425     3.086434    15.719278    60.416009   \n",
      "std        66.965584     3.887704     1.395228     5.077345     8.664170   \n",
      "min         1.000000     0.000000     1.000000    10.000000    28.000000   \n",
      "25%         6.000000     2.000000     2.000000    12.000000    55.000000   \n",
      "50%         9.000000     4.000000     3.000000    15.000000    58.000000   \n",
      "75%       131.000000     6.000000     4.000000    18.000000    70.000000   \n",
      "max       214.000000    24.000000     7.000000    25.000000    87.000000   \n",
      "\n",
      "              rank     statusId         wins         year    circuitId  \n",
      "count  2661.000000  2661.000000  2661.000000  2661.000000  2661.000000  \n",
      "mean      4.339722     1.093950     3.039083  2017.016535    23.121383  \n",
      "std       2.927629     0.964894     4.083984     4.071045    23.821610  \n",
      "min       0.000000     1.000000     0.000000  2011.000000     1.000000  \n",
      "25%       2.000000     1.000000     0.000000  2013.000000     7.000000  \n",
      "50%       4.000000     1.000000     1.000000  2017.000000    14.000000  \n",
      "75%       6.000000     1.000000     4.000000  2021.000000    24.000000  \n",
      "max      17.000000    11.000000    21.000000  2024.000000    80.000000  \n",
      "(2661, 15)\n"
     ]
    }
   ],
   "source": [
    "df_cleaned=df_cleaned[df_cleaned['stop']<=8.5]\n",
    "df_cleaned=df_cleaned[df_cleaned['points']<=25]\n",
    "\n",
    "df_cleaned=df_cleaned[df_cleaned['points']>=10]\n",
    "\n",
    "\n",
    "print(df_cleaned.describe())\n",
    "print(df_cleaned.shape)\n",
    "\n",
    "df_cleaned.to_csv('Processed_DATA_PITSTRAT.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch [1/80], Loss: 1.2219 Validation loss: 0.7194\n",
      "Epoch [2/80], Loss: 0.6303 Validation loss: 0.5163\n",
      "Epoch [3/80], Loss: 0.4642 Validation loss: 0.4251\n",
      "Epoch [4/80], Loss: 0.4011 Validation loss: 0.3894\n",
      "Epoch [5/80], Loss: 0.3801 Validation loss: 0.4093\n",
      "Epoch [6/80], Loss: 0.3347 Validation loss: 0.3931\n",
      "Epoch [7/80], Loss: 0.3370 Validation loss: 0.4041\n",
      "Epoch [8/80], Loss: 0.3441 Validation loss: 0.4117\n",
      "Epoch [9/80], Loss: 0.3220 Validation loss: 0.4310\n",
      "Epoch [10/80], Loss: 0.3203 Validation loss: 0.4029\n",
      "Epoch [11/80], Loss: 0.3090 Validation loss: 0.3816\n",
      "Epoch [12/80], Loss: 0.3011 Validation loss: 0.3862\n",
      "Epoch [13/80], Loss: 0.2925 Validation loss: 0.3797\n",
      "Epoch [14/80], Loss: 0.2902 Validation loss: 0.3935\n",
      "Epoch [15/80], Loss: 0.2819 Validation loss: 0.4040\n",
      "Epoch [16/80], Loss: 0.2764 Validation loss: 0.3964\n",
      "Epoch [17/80], Loss: 0.2954 Validation loss: 0.3923\n",
      "Epoch [18/80], Loss: 0.2813 Validation loss: 0.3875\n",
      "Epoch [19/80], Loss: 0.2745 Validation loss: 0.3885\n",
      "Epoch [20/80], Loss: 0.2768 Validation loss: 0.3882\n",
      "Epoch [21/80], Loss: 0.2695 Validation loss: 0.4045\n",
      "Epoch [22/80], Loss: 0.2745 Validation loss: 0.3968\n",
      "Epoch [23/80], Loss: 0.2658 Validation loss: 0.3889\n",
      "Epoch [24/80], Loss: 0.2791 Validation loss: 0.3900\n",
      "Epoch [25/80], Loss: 0.2631 Validation loss: 0.3913\n",
      "Epoch [26/80], Loss: 0.2584 Validation loss: 0.4026\n",
      "Epoch [27/80], Loss: 0.2736 Validation loss: 0.3810\n",
      "Epoch [28/80], Loss: 0.2703 Validation loss: 0.3960\n",
      "Early stopping at epoch 28 with patience 15 reached.\n"
     ]
    }
   ],
   "source": [
    "#Call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1410e-07, 4.7018e-06, 6.3995e-02, 9.1298e-01, 2.2156e-02, 8.2111e-04,\n",
      "         4.5191e-05, 2.4746e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch from the train_loader\n",
    "inputs, labels = next(iter(train_loader))\n",
    "\n",
    "# Select the first sample (index 0) from the batch\n",
    "input_sample = inputs[4].unsqueeze(0)  # Add batch dimension (from shape [features] to [1, features])\n",
    "label_sample = labels[4].unsqueeze(0)  # Add batch dimension (from shape [num_classes] to [1, num_classes])\n",
    "\n",
    "# Assuming the inputs are already on the correct device (e.g., 'cuda')\n",
    "input_sample = input_sample.to(device)\n",
    "label_sample = label_sample.to(device)\n",
    "\n",
    "# Split the input sample into circuit_onehot, race_onehot, and points_wins\n",
    "circuit_onehot = input_sample[:, :circuit_id_max]\n",
    "race_onehot = input_sample[:, circuit_id_max:circuit_id_max + race_id_max]\n",
    "points_wins = input_sample[:, circuit_id_max + race_id_max:]\n",
    "\n",
    "# Pass the split input sample to the model\n",
    "output_sample = net(circuit_onehot, race_onehot, points_wins)\n",
    "\n",
    "# Output the model's prediction for the sample\n",
    "\n",
    "# Apply Softmax to the output sample along dim=1 (class dimension)\n",
    "softmax = nn.Softmax(dim=1)  \n",
    "output_probabilities = softmax(output_sample)\n",
    "\n",
    "# Round the probabilities to 1 decimal place\n",
    "output_probabilities_rounded = torch.round(output_probabilities * 1000) / 1000\n",
    "\n",
    "print( output_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3960, Macro Accuracy: 0.8705, Micro Accuracy: 0.8705, R²: 0.7802, F1 Macro: 0.5301, F1 Micro: 0.8705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39601948681999655"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            circuit_onehot = inputs[:, :circuit_id_max]\n",
    "            race_onehot = inputs[:, circuit_id_max:circuit_id_max + race_id_max]\n",
    "            points_wins = inputs[:, circuit_id_max + race_id_max:]\n",
    "            \n",
    "            outputs = model(circuit_onehot, race_onehot, points_wins)\n",
    "            loss = criterion(outputs, targets.argmax(dim=1))\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Collecting predictions and targets for metrics\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_targets.extend(targets.argmax(dim=1).cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    macro_accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    micro_accuracy = accuracy_score(all_targets, all_predictions, normalize=True)\n",
    "    \n",
    "    # Calculate R² (though not typical for classification)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1_macro = f1_score(all_targets, all_predictions, average='macro')\n",
    "    f1_micro = f1_score(all_targets, all_predictions, average='micro')\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Macro Accuracy: {macro_accuracy:.4f}, Micro Accuracy: {micro_accuracy:.4f}, R²: {r2:.4f}, F1 Macro: {f1_macro:.4f}, F1 Micro: {f1_micro:.4f}\")\n",
    "    \n",
    "    return avg_val_loss\n",
    "validate_model(net,test_loader,criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Loss: 0.4433\n",
      "Test Set Loss: 0.6268\n",
      "Training Accuracy: 0.8031, Precision: 0.5415, Recall: 0.3757, F1: 0.3783, R2: 0.5661\n",
      "Test Accuracy: 0.7523, Precision: 0.3331, Recall: 0.3269, F1: 0.3270,R2:0.5119\n",
      "Training Accuracy: 0.8031, Precision: 0.8031, Recall: 0.8031, F1: 0.8031, R2: 0.5661\n",
      "Test Accuracy: 0.7523, Precision: 0.7523, Recall: 0.7523, F1: 0.7523,R2:0.5119\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Data preparation\n",
    "input_data = df_cleaned[['circuitId', 'raceId', 'points', 'lap', 'position', 'laps']].values\n",
    "target_data = df_cleaned['stop'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_data[:, 2:] = scaler.fit_transform(input_data[:, 2:])\n",
    "\n",
    "circuit_encoded = OneHotEncoder(sparse_output=False).fit_transform(input_data[:, 0].reshape(-1, 1))\n",
    "race_encoded = OneHotEncoder(sparse_output=False).fit_transform(input_data[:, 1].reshape(-1, 1))\n",
    "\n",
    "X = pd.DataFrame(\n",
    "    torch.cat([\n",
    "        torch.tensor(circuit_encoded, dtype=torch.float32),\n",
    "        torch.tensor(race_encoded, dtype=torch.float32),\n",
    "        torch.tensor(input_data[:, 2:], dtype=torch.float32)\n",
    "    ], dim=1).numpy()\n",
    ")\n",
    "\n",
    "y_encoded = pd.get_dummies(target_data).values\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVM model\n",
    "svm_model = SVC(kernel='rbf', probability=True)  \n",
    "\n",
    "# Training the model\n",
    "svm_model.fit(X_train, y_train.argmax(axis=1))\n",
    "\n",
    "# Predictions\n",
    "train_predictions = svm_model.predict_proba(X_train)\n",
    "train_loss = log_loss(y_train, train_predictions)\n",
    "\n",
    "test_predictions = svm_model.predict_proba(X_test)\n",
    "test_loss = log_loss(y_test, test_predictions)\n",
    "\n",
    "# Print losses\n",
    "print(f'Training Set Loss: {train_loss:.4f}')\n",
    "print(f'Test Set Loss: {test_loss:.4f}')\n",
    "\n",
    "# Class predictions\n",
    "train_pred_labels = svm_model.predict(X_train)\n",
    "test_pred_labels = svm_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "train_accuracy = accuracy_score(y_train.argmax(axis=1), train_pred_labels)\n",
    "test_accuracy = accuracy_score(y_test.argmax(axis=1), test_pred_labels)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "train_precision = precision_score(y_train.argmax(axis=1), train_pred_labels, average='macro')\n",
    "test_precision = precision_score(y_test.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "train_recall = recall_score(y_train.argmax(axis=1), train_pred_labels, average='macro')\n",
    "test_recall = recall_score(y_test.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "train_f1 = f1_score(y_train.argmax(axis=1), train_pred_labels, average='macro')\n",
    "test_f1 = f1_score(y_test.argmax(axis=1), test_pred_labels, average='macro')\n",
    "train_r2 = r2_score(y_train.argmax(axis=1), train_pred_labels)\n",
    "test_r2 = r2_score(y_test.argmax(axis=1), test_pred_labels)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}, R2: {train_r2:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f},R2:{test_r2:.4f}\")\n",
    "\n",
    "train_precision = precision_score(y_train.argmax(axis=1), train_pred_labels, average='micro')\n",
    "test_precision = precision_score(y_test.argmax(axis=1), test_pred_labels, average='micro')\n",
    "\n",
    "train_recall = recall_score(y_train.argmax(axis=1), train_pred_labels, average='micro')\n",
    "test_recall = recall_score(y_test.argmax(axis=1), test_pred_labels, average='micro')\n",
    "\n",
    "train_f1 = f1_score(y_train.argmax(axis=1), train_pred_labels, average='micro')\n",
    "test_f1 = f1_score(y_test.argmax(axis=1), test_pred_labels, average='micro')\n",
    "train_r2 = r2_score(y_train.argmax(axis=1), train_pred_labels)\n",
    "test_r2 = r2_score(y_test.argmax(axis=1), test_pred_labels)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}, R2: {train_r2:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f},R2:{test_r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "# circuit_id_max = 10  # Example value\n",
    "# race_id_max = 5      # Example value\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = MLPModel(circuit_id_max, race_id_max)\n",
    "\n",
    "# # Create random one-hot encoded input data\n",
    "# circuit_onehot = torch.randn(1, circuit_id_max)\n",
    "# race_onehot = torch.randn(1, race_id_max)\n",
    "# points_wins = torch.randn(1, 4)\n",
    "\n",
    "# # Forward pass through the model\n",
    "# output = model(circuit_onehot, race_onehot, points_wins)\n",
    "\n",
    "# # Visualize the computation graph\n",
    "# make_dot(output, params=dict(list(model.named_parameters()))).render(\"mlp_model_graph\", format=\"png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "katana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
